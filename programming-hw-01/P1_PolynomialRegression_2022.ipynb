{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Polynomial-Regression---12-Pts\" data-toc-modified-id=\"Polynomial-Regression---12-Pts-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Polynomial Regression - 12 Pts</a></span><ul class=\"toc-item\"><li><span><a href=\"#a)--Loading-and-Inspecting-the-Data-------2-Pts\" data-toc-modified-id=\"a)--Loading-and-Inspecting-the-Data-------2-Pts-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>a)  Loading and Inspecting the Data   -   2 Pts</a></span></li><li><span><a href=\"#b)-Constructing-the-Vandermonde-Matrix-------2-Pts\" data-toc-modified-id=\"b)-Constructing-the-Vandermonde-Matrix-------2-Pts-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>b) Constructing the Vandermonde Matrix   -   2 Pts</a></span></li><li><span><a href=\"#c)--Moore-Penrose-Pseudoinverse---2-Pts\" data-toc-modified-id=\"c)--Moore-Penrose-Pseudoinverse---2-Pts-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>c)  Moore-Penrose Pseudoinverse - 2 Pts</a></span></li><li><span><a href=\"#d)-Coefficients---2-Pts\" data-toc-modified-id=\"d)-Coefficients---2-Pts-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>d) Coefficients - 2 Pts</a></span></li><li><span><a href=\"#e)-Predictive-Polynomial---2-Pts\" data-toc-modified-id=\"e)-Predictive-Polynomial---2-Pts-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>e) Predictive Polynomial - 2 Pts</a></span></li><li><span><a href=\"#f)-Fitting-different-degree-Polynomials---2-Pts\" data-toc-modified-id=\"f)-Fitting-different-degree-Polynomials---2-Pts-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>f) Fitting different degree Polynomials - 2 Pts</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Regression - 12 Pts\n",
    "==========\n",
    "\n",
    "In this week's programming exercise, you are asked to implement the basic building blocks for polynomial regression step-by-step. We will do the following:\n",
    "\n",
    "\n",
    "- **a)** Load a very simple, noisy dataset and inspect it.\n",
    "- **b)** Construct a design matrix for polynomial regression of degree m: the Vandermonde matrix.\n",
    "- **c)** Calculate the Moore-Penrose pseudoinverse of the design matrix.\n",
    "- **d)** Calculate a vector of coefficients that minimizes the squared error of an n-degree polynomial on our given set of measurements (data).\n",
    "- **e)** Use this coefficient (weight) vector to construct a polynomial that predicts the underlying function the noisy data is drawn from.\n",
    "- **f)** With the work we have done before, we look at a polynomials of different degrees we fit using the provided data.\n",
    "\n",
    "\n",
    "Before you start, make sure that you have downloaded the file *poly_data.csv* from stud.ip and put it in the same folder as this notebook! You are supposed to implement the functions yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T14:22:03.020996Z",
     "start_time": "2021-11-16T14:22:02.430581Z"
    }
   },
   "outputs": [],
   "source": [
    "# the usual imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)  Loading and Inspecting the Data   -   2 Pts\n",
    "------------\n",
    "1. Use the numpy function ```np.loadtxt()``` to load the data from *poly_data.csv* into a variable ```data```. Numpy does not know that we're loading a CSV. Provide the right delimiter as a parameter to ```np.loadtxt()```.\n",
    "\n",
    "\n",
    "2. ```data``` should now be an $n\\times n$ ```ndarray``` matrix. Check whether this is true by using the native python function ```type()``` and the data attribute ```shape``` to obtain the type and dimensions of ```data```.\n",
    "\n",
    "\n",
    "3. The first row and second row correspond to the [independent and dependent variable](https://en.wikipedia.org/wiki/Dependent_and_independent_variables) respectively. Store them in two different, new variables **X** (independent) and **Y** (dependent).\n",
    "\n",
    "\n",
    "4. Use matplotlib's ```scatter()``` to take a look at the data. It has been generated by sampling a function $f$ and adding Gaussian noise:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "y_i &= f(x_i) + \\epsilon \\\\\n",
    "\\epsilon &\\sim \\mathcal{N}(0, \\sigma^2)\n",
    "\\end{align}\n",
    "\n",
    "You can use execute the second cell below to take a look at $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T14:23:35.152696Z",
     "start_time": "2021-11-16T14:23:34.972058Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 1. Read data\n",
    "data = np.loadtxt(\"poly_data.csv\", delimiter=\",\")\n",
    "\n",
    "# 2. Validate data's format\n",
    "data_type = type(data)\n",
    "data_shape = data.shape\n",
    "\n",
    "print(f'data type:\\t{data_type.__name__}')\n",
    "print(f'data shape:\\t{data.shape}')\n",
    "\n",
    "assert(data_type is np.ndarray and data_shape[0] == 2)\n",
    "\n",
    "# 3. Store the two rows in data as two seperate variables: X and Y\n",
    "X, Y = data\n",
    "\n",
    "# 4. Generate a scatterplot using X and Y\n",
    "\n",
    "# setting and labeling your axis\n",
    "plt.ylabel('dependent variable')\n",
    "plt.xlabel('independent variable')\n",
    "plt.xlim((0,10));\n",
    "plt.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T14:24:03.431148Z",
     "start_time": "2021-11-16T14:24:03.315448Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Taking a look at f(x)\n",
    "def target_func(x):\n",
    "    return 1.5*np.sin(x) - np.cos(x/2) + 0.5\n",
    "\n",
    "x = np.linspace(0,10, 101)\n",
    "y = target_func(x)\n",
    "plt.plot(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Constructing the Vandermonde Matrix   -   2 Pts\n",
    "--------\n",
    "\n",
    "In the lecture, you have derived the formula for linear regression with arbitrary basis functions and normal distributed residuals $\\epsilon$. Here, we choose polynomial basis functions and therefore will try and approximate the function above via a polynomial of degree $m$:\n",
    "$$y = \\alpha_0 + \\alpha_1x + \\alpha_2x^2 + \\alpha_3x^3 + \\dots + \\alpha_mx^m + \\epsilon$$\n",
    "Due to our choice of basis functions, this is called polynomial regression.\n",
    "\n",
    "The simplest version of polynomial regression uses monomial basis functions $\\{1, x, x^2, x^3, \\dots \\}$ in the design matrix. Such a matrix is called the [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix) in linear algebra. Implement a function that takes the observed, independent variables $x_i$ stored in **X** and constructs a design matrix of the following form:\n",
    "\n",
    "$$ \\Phi = \\begin{bmatrix} 1 & x_1 & x_1^2 & \\dots & x_1^m \\\\ 1 & x_2 & x_2^2 & \\dots & x_2^m \\\\ 1 & x_3 & x_3^2 & \\dots & x_3^m \\\\ \\vdots & \\vdots & \\vdots & & \\vdots \\\\ 1 & x_n & x_n^2 & \\dots & x_n^m \\end{bmatrix}$$\n",
    "\n",
    "We have provided the function's doc string as well as two quick lines that test your implementation in the notebook cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T14:39:20.477694Z",
     "start_time": "2021-11-16T14:39:20.470705Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def poly_dm(x, m):\n",
    "    \"\"\"\n",
    "    Generate a desing matrix with monomial basis functions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        1-D input array.\n",
    "    m : int\n",
    "        degree of the monomial used for the last column.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    phi : ndarray\n",
    "        Design matrix.\n",
    "        The columns are ``x^0, x^1, ..., x^m``.\n",
    "    \"\"\"\n",
    "\n",
    "    # We need a design matrix with as many rows as entries in x \n",
    "    # and m+1 columns \n",
    "    # Use np.zeros to create a matrix with all entries set to 0\n",
    "    \n",
    "    # first create a row vector with the exponents\n",
    "    exponent = np.arange(m+1)\n",
    "    \n",
    "    # create rows of duplicate x values\n",
    "    base = np.repeat(x.reshape((-1,1)), m+1, axis=1)\n",
    "    \n",
    "    # raise each row of xs to the given exponent\n",
    "    phi = np.power(base, exponent)\n",
    "    \n",
    "    return phi\n",
    "\n",
    "\n",
    "# the lines below test the poly_dm function. no need to understand this code\n",
    "\n",
    "try:\n",
    "    print('poly_dm:',(lambda a=np.random.rand(10):'O.K.'if np.allclose(poly_dm(a,3),np.vander(a,4,True))else'Something went wrong! (Your result does not match!)')())\n",
    "except:\n",
    "    print('poly_dm: Something went horribly wrong! (an error was thrown)')\n",
    "    #print(sys.exc_inf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c)  Moore-Penrose Pseudoinverse - 2 Pts\n",
    "--------\n",
    "\n",
    "According to the lecture, it is quite usefull to calculate the Moore-Penrose pseudoinverse $A^\\dagger$ of a matrix:\n",
    "<br><br>\n",
    "$$ A^\\dagger = (A^T A)^{-1}A^T$$\n",
    "<br>\n",
    "where $A^T$ means transpose of matrix $A$ and $A^{-1}$ denotes its inverse.\n",
    "\n",
    "According to the docstring in the cell below, implement a function that returns $A^\\dagger$ for a matrix $A$, and test your implementation against the small test that is included. \n",
    "\n",
    "For all matrix operations that you need to perform there are numpy functions such as ```np.dot()``` for the dot product between two matrices and matrix attributes like ```matrix.T``` for the transpose of a matrix available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T14:42:58.188767Z",
     "start_time": "2021-11-16T14:42:58.158847Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def pseudoinverse(A):\n",
    "    \"\"\"\n",
    "    Compute the (Moore-Penrose) pseudo-inverse of a matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : (M, N) array_like\n",
    "      Matrix to be pseudo-inverted.\n",
    "      \n",
    "    Returns\n",
    "    -------\n",
    "    A_pinverse : (N, M) ndarray\n",
    "      The pseudo-inverse of `a`.\n",
    "    \"\"\"\n",
    "    # Hint: Cast A as matrix and check numpy matrix attributes at \n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
    "    # Furthermore, check out numpy's @ operator\n",
    "    M = np.asmatrix(A)\n",
    "    \n",
    "    # Cast to matrix again to make Inverse available\n",
    "    A_pinverse = np.asmatrix(A.T @ A).I @ A.T\n",
    "    \n",
    "    return np.array(A_pinverse)\n",
    "\n",
    "# the lines below test the pseudo_inverse function\n",
    "try:\n",
    "    print('pseudo_inverse:',(lambda m=np.random.rand(9,5):'Good Job!'if np.allclose(pseudoinverse(m),np.linalg.pinv(m))else'Not quite! (Your result does not match!)')())\n",
    "except:\n",
    "    print('pseudo_inverse: Absolutely not! (an error was thrown)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Coefficients - 2 Pts\n",
    "-------\n",
    "To estimate the parameters $w_i$ up to a chosen degree $m$, we use the vector $\\vec{w}^{\\,}$ containing all the $w_i$ and solve the following formula:\n",
    "\\begin{align}\n",
    "y &= \\Phi \\vec{w}^{\\,} \\\\\n",
    "\\vec{w}^{\\,} &= \\Phi^\\dagger y\n",
    "\\end{align}\n",
    "where $\\Phi$ is the design matrix and $\\Phi^\\dagger$ its pseudoinverse and $y$ is the vector of dependent variables we observed in our dataset and stored in **Y**.\n",
    "\n",
    "Implement a function that calculates $\\vec{w}^{\\,}$ according to the docstring given below. Again, a short test of your implementation is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T14:44:58.542877Z",
     "start_time": "2021-11-16T14:44:58.528917Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def poly_regress(x, y, deg):\n",
    "    \"\"\"\n",
    "    Least squares polynomial fit.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array, shape (M,)\n",
    "        x-coordinates of the M sample points.\n",
    "\n",
    "    y : array, shape (M,)\n",
    "        y-coordinates of the sample points.\n",
    "    \n",
    "    deg : int\n",
    "        Degree of the fitting polynomial.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape (deg+1,)\n",
    "        Polynomial coefficients, highest power last.\n",
    "    \"\"\"\n",
    "    pi = pseudoinverse(poly_dm(x, deg))\n",
    "    \n",
    "    w = pi @ y\n",
    "    \n",
    "    return w\n",
    "\n",
    "\n",
    "# the lines below test the poly_regress function\n",
    "try:\n",
    "    print('poly_regress:',(lambda a1=np.random.rand(9),a2=np.random.rand(9):'Ace!'if \n",
    "                           np.allclose(poly_regress(a1,a2,2),np.polyfit(a1,a2,2)[::-1])else'Almost! (Your result does not match!)')())\n",
    "except:\n",
    "    print('poly_regress: Not nearly! (an error was thrown)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Predictive Polynomial - 2 Pts\n",
    "--------\n",
    "The last function we will write will use the vector of coefficients we can now calculate to construct a polynomial function and evaluate it at any point we choose to. Remember, the form of this polynomial is given by:\n",
    "<br><br>\n",
    "$$y = w_0 + w_1x + w_2x^2 + w_3x^3 + \\dots + w_mx^m$$\n",
    "\n",
    "This is the model we assumed above, but we do not need to include the noise term here! Again, the function is specified in a docstring and tested in the little {try - catch} block below. *Hint:* The order of the polynomial you need to calculate is (indirectly!) given by the length of **w**, the number of coefficients. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T15:14:10.703323Z",
     "start_time": "2021-11-16T15:14:10.696341Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def polynom(x, w):\n",
    "    \"\"\" Evaluate a polynomial.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1d array\n",
    "        Points to evaluate.\n",
    "    \n",
    "    w : 1d array\n",
    "        Coefficients of the monomials.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : 1d array\n",
    "        Polynomial evaluated for each cell of x.\n",
    "    \"\"\"\n",
    "    # create exponents\n",
    "    exponent = np.arange(len(w))\n",
    "    # repeat x for each exponent\n",
    "    base = np.repeat(x.reshape((-1,1)), len(exponent), axis = 1)\n",
    "    pwr = np.power(base, exponent)\n",
    "    # reshape to make w and pwr compatible\n",
    "    y = pwr @ w.reshape(-1,1)\n",
    "    \n",
    "    # cast to 1D-Array\n",
    "    y = y.flatten()\n",
    "    return y\n",
    "\n",
    "# the lines below test the polynom function\n",
    "try:\n",
    "    print('polynom:',(lambda a1=np.random.rand(9),a2=np.random.rand(9):'OK'if np.allclose(polynom(a1,a2),np.polyval(a2[::-1],a1))else'Slight failure! (Your result does not match!)')())\n",
    "except:\n",
    "    print('polynom: Significant failure! (an error was thrown)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Fitting different degree Polynomials - 2 Pts\n",
    "------\n",
    "__\"f\"__, as in finally. We can now use all the functions we have written above to investigate how well a polynomial of a degree $m$ fits the noisy data we are given. For $m \\in \\{1,3,10\\}$, estimate a polynomial function on the data. Evaluate the three functions on a vector of equidistant points between 0 and 10 (*linearly spaced*). Additionally, plot the original target function $f(x)$, as well as the scatter plot of the data samples. Make sure every graph and the scatter appear in the same plot. Label each graph by adding a label argument to the ```plt.plot``` function. This allows the use of the ```legend()``` function and makes the plot significantly more understandable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T15:19:22.892877Z",
     "start_time": "2021-11-16T15:19:22.660499Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "degrees = (1, 3, 10)\n",
    "\n",
    "x = np.linspace(0,10,101)\n",
    "y = target_func(x)\n",
    "plt.scatter(X,Y)\n",
    "plt.plot(x,y,label='target function',lw=4)\n",
    "\n",
    "for d in degrees:\n",
    "    w = poly_regress(x, y, d)\n",
    "    y_fit = polynom(x, w)\n",
    "    plt.plot(x, y_fit, label=f\"regression deg {d}\")\n",
    "    \n",
    "plt.xlim((0,10))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell is just for easier correction by the tutors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T15:19:28.914321Z",
     "start_time": "2021-11-16T15:19:28.901329Z"
    }
   },
   "outputs": [],
   "source": [
    "# b) the lines below\n",
    "try:\n",
    "    print('poly_dm:',(lambda a=np.random.rand(10):'O.K.'if np.allclose(poly_dm(a,3),np.vander(a,4,True))else'Something went wrong! (Your result does not match!)')())\n",
    "except:\n",
    "    print('poly_dm: Something went horribly wrong! (an error was thrown)')\n",
    "    print(sys.exc_inf())\n",
    "\n",
    "# c) the lines below test the pseudo_inverse function\n",
    "try:\n",
    "    print('pseudo_inverse:',(lambda m=np.random.rand(9,5):'Good Job!'if np.allclose(pseudoinverse(m),np.linalg.pinv(m))else'Not quite! (Your result does not match!)')())\n",
    "except:\n",
    "    print('pseudo_inverse: Absolutely not! (an error was thrown)')\n",
    "\n",
    "# d) the lines below test the poly_regress function\n",
    "try:\n",
    "    print('poly_regress:',(lambda a1=np.random.rand(9),a2=np.random.rand(9):'Ace!'if \n",
    "                           np.allclose(poly_regress(a1,a2,2),np.polyfit(a1,a2,2)[::-1])else'Almost! (Your result does not match!)')())\n",
    "except:\n",
    "    print('poly_regress: Not nearly! (an error was thrown)')\n",
    "    \n",
    "# e) the lines below test the polynom function\n",
    "try:\n",
    "    print('polynom:',(lambda a1=np.random.rand(9),a2=np.random.rand(9):'OK'if np.allclose(polynom(a1,a2),np.polyval(a2[::-1],a1))else'Slight failure! (Your result does not match!)')())\n",
    "except:\n",
    "    print('polynom: Significant failure! (an error was thrown)')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
